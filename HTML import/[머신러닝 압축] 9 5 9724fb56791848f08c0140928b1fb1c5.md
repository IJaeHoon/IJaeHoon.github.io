 

# [머신러닝 압축] 9/5

순환 신경망

```
from tensorflow.keras.datasets import imdb

(train_input, train_target), (test_input, test_target) = imdb.load_data(
    num_w


ords=300)
```

# num_words=300은, 문장 단어가 300개가 아니라, 어휘가 300개

단어를 스칼라 값으로 숫자로 변환하여 표현한다,

딥러닝 모델에 들어갈땐 벡터로 바꿔야한다

[![](HTML%20import/Attachments/Untitled%2066.png)](Untitled%2066.png)

각 단어의 리뷰의 갯수

히스토그램 ← 구간의 도수를 구하는거

막대그래프 ← 값을 찍는거고

시각적인 차이,

간격이 없다 → 히스토그램

막대그래프는 → 간격이 있어야한다

긍정 부정을 자연어처리하는게 아직도 힘들다

[![](Untitled%2067.png)](Untitled%2067.png)

ㄴ100개보다 길면 끊어버리고 100개가 안되면 이후 문장은 다 0으로 처리한다

원래 RNN은 자연어 처리하는데 부적합해 > I hate you

RNN ⇒ 벡터 = 숫자열

# 벡터입니다 - 숫자 1개가 아닌 숫자 여러개로 되어있다

I = 숫자 여러개, 스펠링은 여러개라도 다 300개, I도 You도 Love도 다 300개

웨이티드썸 → 웨이트를 곱한 합

입력값에 가중치 곱해서 더한 것 → 그리고나서 액티베이션펑션에 통과시키는 것 액티베이션펑션이 뭐냐에 따라 회귀/분류

렐루 액티베이션함수 비슷한거 → 리키렐루, 셀루

리키렐루 → 조금 새는 렐루

렐루의 가장 큰 단점 = 미분이 안됨, → 꺾이는 부분 사인함수로 바꿔서 사인함수+렐루 = 미분되는 렐루

모든 공학은 그렇지만 특히 컴퓨터공학은 물리학이 신이 만든거라면 컴퓨터공학은 인간이 만든 것, 이해가 잘 안됨 이해안돼도 성능이 잘 나옴

미분했을때 최대의 값 1/4 = 0.25

위드임베딩 = 벡터쓰긴하는데 1이 여러개인 구조로

시간적 순서에 대한 웨이트

부정은 -값 / 애매하면 0에 가깝게 / 긍정은 +값 / 그래서 마이너스 값이 존재하는 탄젠트를 쓰는거

입력 벡터 > 들어간 값 > 웨이티드 썸 > 웨이티드썸에다가 뺑돌아 다시 들어오는 구조 순서보정값하나 준거,

RNN 출력의 형태 여러개, 그걸 후처리로 코딩하는거

이 구조를 다대일 구조라한다

입력은 여러개인데 출력이 1개다, RNN은 CNN과 다르게, 출력적 특성이 여러개가 있다.

번역 → 오토인코더 → 얘도 퍼셉트론 구조

지금의 모든 번역은 트랜스포머 모델을 쓴다 → 어텐션 쓴다

벡터 단순하게 말하면, 하나의 정보를 숫자 여러개로 표현한거

p498

dlalwl32 wkdusdj 64 128

모든 퍼셉트론은 어차피 웨이티드썸하는것

단어임베딩

# 한국어 임베딩 - 이기창 저자

한국어 임베딩에 대해 공부하기 좋은 책

word2vec.kr

#원핫인코딩의 단점

- 너무 많아, 이번엔 단어 1개를 위해 300개 사전은 40만개 필요할 것, 메모리낭비

- 아무 의미없이 표현됨, 유사하지않은데 유사하다고 기계는 생각하기도 캣을보고 큐티라고함

#워드인베딩

값을 조정하기때문에 실수값을씀, 1만쓰지않음, 해당 벡터와 비슷한 벡터라면 비슷한 값이 구성됨

입력과 출력

모든 퍼셉트론

입력값 벡터 (1개가 아니라면)

출력값 숫자 출력 1개

# LSTM

기억하고 싶은 단어를 좀 더 길게 기억하게 하는 것

특징을 찾기위한 숫자놀음

이미지는 밝기

이건 텍스트 벡터의 숫자 강조

숫자키우려고 웨이트 곱해주고

키운걸 뒤에 다른 숫자값을 넣어줘서

# 머신러닝 = 입력구조 / 출력구조

이 두개는 어떻게 되는지는 계속 생각을 해야한다

# 임베딩은 딥러닝에 퍼셉트론 구조로 쓴게 아니다 ≠

임베딩은 가장 잘 표현하는 16개의 숫자만 찾으면 된다

8개의 노드

오차함수 루스펑션 - 바이너리+ 크로스 엔트로피

목표

50살 넘어서 코딩하는건

나의 스타일로 해야지

남이 시키는건 하기 힘듦,

교수님 권고

[권고 1]

50세쯤되면 회사에 있어도코더보단 매니저 시켜, 힘들다 하기도 싫고

그러므로 끝까지 남되, 45세 이전에 관둔 후

용역을 받아서 하던가 어느 하나 특출난 영역을

구축해서 놀면서 한단 마인드로 조기 파이어한다는 마인드도 좋다

[권고 2]

아니면, 외국물도 먹어봐라 아마존에서 3년 일해봐라

원격으로 코딩테스트 다 한다

오히려 외국은 뼈를 묻겠습니다, 안 받아줌,

3년 일하고 창업할거에요< 오히려 받아줌

창업> 나만 할 수 있는 것 하는게 맞다

씨스코 ccna cccp 프로페셔널

콜 부름 3시간 ~ 6시간 걸립니다

시간 당 60만원 ~ 100만원

처치해주고 180받는거 < 요샌 많음

프로그래머도 ‘나만이 할 수 있는 것’ 있으면

20억 마련하고 45세에 관두고 놀면 됨

미래 디자인을 할 것, 당장 먹고 사는 것도 중요하지만

보통은 45세 넘어서야 출구전략을 생각함,

30세에 미리 디자인 생각 해두는게

수월하다 그게 아니면 평생 일하면서 살 것인지,

45세에 관둘 생각으로 연봉 2억 받으면 15년 일할듯

레이어가 많으면 많을수록 성능이 안 나아질 수가 없다

RNN(Recurrent Neural Network)은 딥러닝 모델 중 하나로, 순차적인 데이터를 처리하기 위해 설계된 신경망입니다. RNN은 시간에 따른 데이터나 순서가 있는 데이터를 처리하는 데 강력한 도구로 사용됩니다.

**RNN(Recurrent Neural Network)**:

- RNN은 순서가 있는 데이터를 처리할 때 사용됩니다. 예를 들어, 문장, 시계열 데이터, 음성 데이터 등을 처리하는 데 적합합니다.

- RNN은 순차적으로 데이터를 입력받으며, 이전 단계의 출력을 현재 단계의 입력으로 사용합니다. 이러한 반복 구조를 통해 시퀀스 데이터의 패턴을 학습할 수 있습니다.

- RNN은 각 단계에서 가중치와 상태(state)를 공유하여 이전 정보를 기억하고 새로운 정보와 결합하여 출력을 생성합니다.

- 하지만 RNN은 긴 시퀀스를 처리할 때 그래디언트 소실 문제(vanishing gradient problem)가 발생할 수 있어서, 긴 시퀀스를 처리하는 데 어려움이 있습니다.

**LSTM(Long Short-Term Memory)**:

- LSTM은 RNN의 한 종류로, 장기 의존성(long-term dependency)을 효과적으로 다루기 위해 고안되었습니다.

- LSTM은 기본 RNN과 달리, 게이트(gate) 메커니즘을 사용하여 정보를 저장하고 삭제하는 방법을 학습합니다.

- LSTM은 장기 의존성을 가진 시퀀스 데이터를 처리할 때 매우 효과적이며, 긴 시퀀스에 대한 정보를 오랫동안 기억할 수 있습니다.

**GRU(Gated Recurrent Unit)**:

- GRU도 LSTM과 유사한 게이트 메커니즘을 사용하는 RNN의 변종입니다.

- LSTM보다 간단한 구조를 가지고 있으며, 더 적은 파라미터로 학습할 수 있습니다.

- GRU는 기본적인 시퀀스 모델링 작업을 수행하는 데 효과적이며, 상대적으로 더 빠르게 훈련될 수 있습니다.

간단히 말하면, RNN은 순차적인 데이터를 처리하기 위한 신경망 구조이며, LSTM과 GRU는 RNN의 향상된 버전으로, 장기 의존성 문제를 해결하고 시퀀스 데이터를 더 효과적으로 처리하기 위해 고안되었습니다. LSTM과 GRU는 다양한 자연어 처리 및 시계열 예측 작업에서 널리 사용되고 있습니다.

GRU는 LSTM의 아류다,

GRU는 해외논문 등에선 잘 설명되진않음, 국뽕인 부분이 있음

이런 짓 하는 이유 = 왜 곱했고 왜 더했나 공부하면서 생각

디시전트리 = 맥스렝스 왜하나 < 과적합 피하려고