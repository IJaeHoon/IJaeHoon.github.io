 

# [머신러닝 압축] 8/30

AUTODESK 사이트 (공룡)

이미지

이미지쪽에서 저걸만들면 굉장히 수월한작업가능 소득

이미지 전처리할때 활용하면됨

군집 알고리즘 < 아웃라이어 찾아낼때 기가막히게 찾아낸다

=이상치를 걸러내는걸 목표로 삼아야하면, 이상치를 집어넣고 군집으로 돌려보고 넣고

=이상치가 별상관없으면 그냥 돌려보고

분류학습 전처리할때 군집알고리즘 쓰는 경우 늘어났음

얘는 애초에 분류를 목적으로 쓰는 모델이 아님

넷플릭스 추천시스템 등 K-means로 복잡하게 설계된 것,

추천시스템 기능 = K-means

현업에선 레이블이 없는 데이터를 다루는 경우가 많다, 시중에는 책 한권 있음 스스로 공부해야함,

비지도학습은 짧게 만들기 어려움 단기강좌 만들기힘듦

PCA 데이터를 줄여서 키 몸무게 2개일때보다,

bmi지수로 만들면 결과예측이 더 정확하지않을까?

카드사기 card fraud 정형데이터

원본데이터 1000개 중 500개로 압축하면, 99%는 일치,

10000개를 50개로 PCA를 해도 잘 복원된 예시

★정형데이터 속성이 30개인데, PCA로 50개 만들고 오히려 속성을 늘인 뒤 사용하는 경우도 있음

PCA가 공통된 속성을 뽑아내데 탁월하다보니, 연산속도를 포기하더라도 속성을 늘리는 경우이다.

시각장애인들 이미지가 크겠지 저 컴퍼넌트 처리하는걸

피시에이처리해버리고 모델로 집어넣으면

굉장히 가벼운 모델을 써도 생각보다 빨리 처리할 수 있음

특징을 잘 추출한다, ← 정형데이터 특징 추출된거같았지만 PCA쓴다

MNIST Modified National Institute of Standards and Technology Databast

# 서비스할만하다 싶으면 출시하는거다

이미 선점해두면 사람들은 바꾸기 어려워진다

구글 → 텐서플로 / 텐서 앤 테라스 라이브러리

페이스북 → 파이토치 라이브러리

영어와 일본어 정도의 차이는 아니고.. 공부는 해야함 한달정도 걸릴 것,

아마존 → Builders? 빌더즈

아파치 → Mx?Net

마이크로소프트 → CNDK

각자의 라이브러리가 있음

회귀일 경우에는 알스퀘어값을 던져줌

분류일 경우에는 애큐러시값을 던져줌

# ★

# ★ 인공신경망

프랭크 로젠블럿 → 인간의 신경세포 Human Neuron ⇒ perceptron

액티베이션펑션

퍼셉트론

입력받는건다같고가중합구하는거도같은데

f가머냐에따라서뱉는게다달라짐

퍼셉트론 → 숫자 여러개 받아서 하나의 숫자를 뱉는다, 그냥 뱉는건 아니고 숫자 사이에서 그 숫자들 각각의 중요도를 찾는 것이다

중요한걸 찾아야함, 3개 정보를 처리해서 다음 놈에게 뱉어줘야하는데 다음 놈이 야 그거 별로야하면 잘못처리된거,

비선형회귀랑 똑같은거 로지스틱회귀랑 똑같은거

퍼셉트론이 리니어리그레이션이랑 같은 기능하려면?

소프트맥스<분류

시그모이드<분류

액티베이션함수

도형이 그려지는데 도형을 맞춤 O X △

뉴욕타임즈 대서특필

퍼셉트론 최초 성공 사례

액티베이션함수

perceptron -저서, 가장 수학적으로 엄밀하게 설명한 책, 근데 번역서가 없음

마빈 민스키

퍼셉트론을 가장 완벽하게 설명한 책이지만, 처음부터 끝까지 수학적으로 까는 것이다, 기호주의자 마빈 민스키

x or 입력이 다르게 들어가야 출력이 1이 나옴 퍼셉트론은 x or을 해결못한다.

# 기호주의자 vs 연결주의자

# 기호주의자: 인공지능은 죽었다 깨어나도 사람이 될 수 없다

둘이 섞어야한다 주장, 요슈아 벤주어

퍼셉트론을 가장 완벽하게 설명한 책이지만, 처음부터 끝까지 수학적으로 까는 것이다, 기호주의자 마빈 민스키

x or 입력이 다르게 들어가야 출력이 1이 나온다

x or도 안된다

그렇지만 마빈 민스키도 ‘나는 관대하다

다음 다음 조금 복잡하게 만들면 x or 처리 가능해 향후 50년동안 이 방법을 수학적으로 가능하나, 연결해서 가능하나,

엄격하고 체계적으로 수학적으로 모델링하는건 향후 50년동안 안나올거다’ 라고 했음 → 20년 후 재프리 힌튼이 증명해냈다

마빈 민스키

퍼셉트론을 수학적으로 너무 완벽하게 분석하고, 프랭크 로젠 블럿이 충격받고, 이후 기호주의자들한테 밀리기 시작

향후 40년동안 디질거임< 제자 데리고 노력했지만 4년 후 1968년 죽음 태양은 가득히처럼 죽음 실족사했지만, 스트레스받아 자살한것으로 추정

20년동안 칼을 갈고 재프리 힌튼이 해결했음

개 고양이 // 마빈 민스키 曰 연결주의자들, 개 고양이 하나 구분못하잖아 네모세모밖에 구분 못해 라고해서

그래서 개 고양이 해결한 얀 르쿤, 손글씨 넣으면서 개 고양이 해결한거 보여줌

재프리힌튼이 자동화도 가르쳐주고 컴퓨터도 발전, 현재는 연결주의자가 득세

ㄴ알쓸신잡기술

퍼셉트론이 회귀식이랑 똑같이 생김, ★액티베이션 펑션이 없으면 똑같은거,

딥러닝은 기본적으로 직선, 직선을 이용하긴해, 근데 오차를 줄이기위해서 직선을 계단식으로 한다 /-/—/

이게 딥러닝의 수학저인 펀더멘탈임, 멀리서보면 직선인데, 가까이서보면 직선이 아니다.

퍼셉트론노드의출력음다음퍼셉트론에집어넣고, 집어넣고, 돋보기로보면 직선1개가아님, 다중직선, 다중의직선을 들음,

집단지성 개념을 이용, ← 니네 마빈 민스키가 여러개 연결하면 된다던데 < 말 그대로 여러개 이용, 단 수학적으로

첫레이어인풋 = 허리둘~보고 = 대답

두번째 레이어는 첫번째보고 = 대답

그렇게 가중치를 줘야해, 이게 딥러닝의 구조임

첫레이어 고혈압확률 xx.xx%

2레이어, 원본몰겠고 1레이어 보고 믿고(신뢰도o 가중치x)

딥 뉴럴 네트워크 ≠ 딥 빌리프 네트워크, 그 전 레이어를 얼마나 신뢰하느냐

2레이어 오답이면 3레이어는 2레이어 신뢰도 낮추고, 맞다한레이어신뢰도 높임, 웨이트값조정해, 원본데이터

백프로파게이션(역전파(Backpropagation) - 쵸코쿠키의 연습장) / 역전파 → W값이 계산될것 ← 마빈 민스키가 이게 안될거라고 한거 / 재프리힌튼이 이걸 해낸거, 체인을 이용해서 연쇄미분하면된다 다중미분으로 성공

가중치줄때 신뢰도를 측정하고~ 그전에 누군가가잘해준거고 그전전에누군가가 잘해준거고 f(x) = gx zx tx 내안에 너있다 그안에 너있다 <이거 한방에 미분할수만 있으면 돼요

장병철교수님 25년전, 교수님이 레이어3개 노드2~3개 손계산→ 6시간주고 하라했다함, 계산기는 써도된다했다함

`dense` `**=**` `keras``**.**``layers``**.**``Dense(10, activation``**=**``'softmax', input_shape``**=**``(784,))`

하나의 레이어에 퍼셉트 10개 밖에 없다

f = softmax

⇒ logistic Function

종류, 클래스 10개라 10

softmax = 모든 더한 값을 분모에 넣고 / 각각 분자값을 한게 소프트맥스, 전체가 1이된다

그래서 사실 이건 딥러닝이 아님, 뉴럴 네트워크 구조, 레이어 1개밖에없다, 인풋이자 아웃풋

스칼라 값 하나, 퍼셉트가 아냐 , 이게 들어간 첫번째 인풋이 1번째 레이어라고함 원칙,

그 다음부터가 히든 레이어, 마지막 출력을 담당하는 레이어가 아웃풋 레이어 아웃풋레이어는 몇개던, 아웃풋 레이어

댄스는 모든 노드가 연결되는게 댄스라그래, 댄스가 다 안들어가는 노드도 있는데 2일 뒤 등장할거

모든 딥러닝 훈련을 해서 찾으려하는 것은 = 가중치

가중치 찾는게 훈련, 맞는 답을 뱉는 ‘w’

지피티도 저 구조임, GPT는 한번 훈련해서 찾아야할 w갯수가 7천억개 들어감,

내가 원하는 답이 안나오면 고쳐야하는데, 7천억개를 미분해서 고쳐야함 문장으로 치면 몇조개가 될것,

웨이트 6천억번하면 1핫돌아간거

선형회귀에서의 루스펑션이름이 MSE <로지스틱리그레이션에선 로그루스, < 사이킥런에서

로그루스 == 카테고리컬 크로스엔트로피

P log p - 1-p log 1- asd

원핫인코딩